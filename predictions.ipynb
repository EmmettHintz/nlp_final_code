{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 Large Prediction Script\n",
    "## Optimized for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Step 0: Disable W&B Logging\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B before any imports\n",
    "\n",
    "# ================================\n",
    "# Step 1: Install and Import Libraries\n",
    "# ================================\n",
    "\n",
    "# Install required libraries\n",
    "!pip install --upgrade transformers datasets scikit-learn imbalanced-learn\n",
    "\n",
    "# Import libraries and verify versions\n",
    "import transformers\n",
    "import datasets\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Ensure PyTorch uses the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "# Change directory to your project folder in Google Drive (if applicable)\n",
    "# %cd /content/drive/MyDrive/your_project_folder\n",
    "\n",
    "# ================================\n",
    "# Step 2: Load the Data\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv('train_data.tsv', sep='\\t')\n",
    "val_df = pd.read_csv('validation_data.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('test_data.tsv', sep='\\t')\n",
    "\n",
    "# ================================\n",
    "# Step 3: Prepare the Datasets\n",
    "# ================================\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df = df.copy()\n",
    "    # Ensure 'text' is a single string, not a list\n",
    "    df['text'] = df['text'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_dataframe(train_df)\n",
    "val_df = preprocess_dataframe(val_df)\n",
    "test_df = preprocess_dataframe(test_df)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# ================================\n",
    "# Step 4: Encode the Labels and Compute Class Weights\n",
    "# ================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Identify unique class labels\n",
    "class_labels = sorted(train_df['label'].unique())\n",
    "print(f\"Unique class labels: {class_labels}\")\n",
    "\n",
    "# Create label to ID and ID to label mappings\n",
    "label_to_id = {label: idx for idx, label in enumerate(class_labels)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(f\"Label to ID mapping: {label_to_id}\")\n",
    "print(f\"ID to Label mapping: {id_to_label}\")\n",
    "\n",
    "# Encode labels in the dataset\n",
    "def encode_labels(example):\n",
    "    example['labels'] = label_to_id[example['label']]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Compute class weights based on the training set\n",
    "train_labels = dataset['train']['labels']\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# Convert class weights to a tensor and move to the appropriate device\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class weights: {class_weights_tensor}\")\n",
    "\n",
    "# ================================\n",
    "# Step 5: Tokenize the Text\n",
    "# ================================\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ================================\n",
    "# Step 6: Set Format for PyTorch\n",
    "# ================================\n",
    "\n",
    "dataset.set_format(\n",
    "    type='torch',\n",
    "    columns=['input_ids', 'attention_mask', 'labels']\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 7: Initialize the Model\n",
    "# ================================\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(class_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'microsoft/deberta-v3-large',\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "# ================================\n",
    "# Step 8: Define the Metrics\n",
    "# ================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels_true = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels_true, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels_true, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# ================================\n",
    "# Step 9: Set Up Training Arguments\n",
    "# ================================\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./deberta_results',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=18,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./deberta_logs',\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,                    # Enable mixed precision\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],                  # Disable all logging integrations\n",
    "    logging_steps=50,             # Ensure frequent logging\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 10: Define a Custom Trainer with Class Weights\n",
    "# ================================\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Initialize the custom trainer with class weights\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 11: Train the Model\n",
    "# ================================\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ================================\n",
    "# Step 12: Evaluate the Model\n",
    "# ================================\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "eval_results = trainer.evaluate(eval_dataset=dataset['validation'])\n",
    "print(eval_results)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "test_results = trainer.evaluate(eval_dataset=dataset['test'])\n",
    "print(test_results)\n",
    "\n",
    "# ================================\n",
    "# Step 13: Make Predictions on the Test Set\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predictions, labels, metrics = trainer.predict(dataset['test'])\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert numeric labels back to text labels\n",
    "predicted_labels = [id_to_label[pred] for pred in preds]\n",
    "true_labels = [id_to_label[label] for label in labels]\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "test_df['predicted_label'] = predicted_labels\n",
    "test_df['true_label'] = true_labels\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "test_df.to_csv('deberta_test_predictions.csv', index=False)\n",
    "\n",
    "# ================================\n",
    "# Step 14: Analyze the Results\n",
    "# ================================\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_df['true_label'],\n",
    "    test_df['predicted_label'],\n",
    "    target_names=class_labels  # Ensure this matches the number of classes\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "\n",
    "\n",
    "predictions = np.argmax(predictions, axis=1)  # Raw predictions to class IDs\n",
    "true_labels = labels  # True class IDs\n",
    "\n",
    "# Map IDs back to class labels\n",
    "predicted_labels = [id_to_label[pred] for pred in predictions]\n",
    "true_labels_text = [id_to_label[true] for true in true_labels]\n",
    "\n",
    "# Get class names\n",
    "class_labels = sorted(list(set(true_labels_text)))\n",
    "\n",
    "# ============================\n",
    "# Step 1: Confusion Matrix Heatmap\n",
    "# ============================\n",
    "cm = confusion_matrix(true_labels_text, predicted_labels, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# Step 2: Precision, Recall, and F1-Score Bar Chart\n",
    "# ============================\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "    true_labels_text, predicted_labels, labels=class_labels, zero_division=0\n",
    ")\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "bar1 = ax.bar(x - width, precision, width, label='Precision')\n",
    "bar2 = ax.bar(x, recall, width, label='Recall')\n",
    "bar3 = ax.bar(x + width, f1_score, width, label='F1-Score')\n",
    "\n",
    "for bars in [bar1, bar2, bar3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Class Labels')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Performance Metrics per Class')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# Step 3: Class Distribution Pie Chart\n",
    "# ============================\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.pie(support, labels=class_labels, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen', 'salmon'])\n",
    "ax.set_title('Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print updated classification report\n",
    "print(\"Updated Classification Report:\")\n",
    "print(classification_report(true_labels_text, predicted_labels, target_names=class_labels))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
